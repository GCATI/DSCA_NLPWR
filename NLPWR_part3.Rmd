---
title: "NLP with R - Part 3: topic modelling with LDA"
author: Sonia Mazzi<br> [ONS - Data Science Campus](https://datasciencecampus.ons.gov.uk)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    fig_caption: false
    fig_width: 11
    fig_height: 6
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true 
    theme: cosmo
    df_print: paged
---

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri("pics/dsclogo.png"),
               alt = "logo", 
               style = "position: absolute; width: 180px; top:75px; right:10px; padding: 10px")
```



**Required libraries**

We will first load the libraries we will be using in what follows.

If you don't have the libraries installed you may need to execute this first

```{r eval=FALSE}
install.packages(c("readr", "dplyr", "tidyr", "stringr", "ggplot2", "kableExtra", "formattable", "gridExtra", "tidytext", "textdata", "magick", "circlize", "topicmodels", "tm"))
```

Now you can load the libraries

```{r message=F, warning=F}
library(readr)# read text files
library(dplyr) #data manipulation
library(tidyr)
library(stringr) #manipulate strings
library(ggplot2) #visualizations
library(kableExtra)
library(formattable)
library(gridExtra) #viewing multiple ggplots in a grid
library(tidytext) #text mining
library(magick)
library(circlize)
library(topicmodels)
library(tm)
```

<br>

**ggplot pre-set theme**

```{r}
theme_prince <- function(aticks = element_blank(),
                         pgminor = element_blank(),
                         lt = element_blank(),
                         lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), #Center the title
        axis.ticks = aticks, #Set axis ticks to on or off
        panel.grid.minor = pgminor, #Turn the minor grid lines on or off
        legend.title = lt, #Turn the legend title on or off
        legend.position = lp) #Turn the legend on or off
}
```

**Table styling pre-set**

```{r}
#this function is to print a table using kable and kableExtra
my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                full_width = FALSE)
}
```

**Define some colors to use throughout**

```{r}
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00")
```


# Topic modelling

In 2016, Layman et.al report in ![paper](https://ieeexplore.ieee.org/document/7832910?part=1)

>Problem reports at NASA are similar to bug reports: they capture defects found during test, post-launch operational anomalies, and document the investigation and corrective action of the issue. These artifacts are a rich source of lessons learned for NASA, but are expensive to analyze since problem reports are comprised primarily of natural language text [...] We collected 16,669 problem reports from six NASA space flight missions and applied Latent Dirichlet Allocation topic modeling to the document corpus. We analyze the most popular topics within and across missions, and how popular topics changed over the lifetime of a mission. We find that hardware material and flight software issues are common during the integration and testing phase, while ground station software and equipment issues are more common during the operations phase. 



The main goal of topic modeling is to find significant thematically related terms (topics) in unstructured text data by measuring patterns of word co-occurrence. 

Some applications of topic modeling are:

* Document summaries: Use topic models to understand and summarize scientific articles enabling faster research and development. The same applies to historical documents, newspapers, blogs, and even fiction.

* Text classification: Topic modeling can improve classification by grouping similar words together in topics rather than using each word as an individual feature.

* Recommendation Systems: Using probabilities based on similarity, you can build recommendation systems. You could recommend articles for readers with a topic structure similar to articles they have already read.

<br>

# Latent Dirichlet Allocation (LDA)

The basic components of topic models are **documents**, **terms**, and **topics**. 

A popular machine learning method used for topic modelling is Latent Dirichlet Allocation (LDA). 
LDA  is an unsupervised machine learning method which discovers different topics underlying a collection of documents or corpus, where each document is a collection of words. 

LDA makes the following assumptions:

* Every document is a combination of one or more topic(s).

* Every topic is a mixture of words.

In this sense, documents can overlap in terms of topics, topic categories are not mutually exclusive, which is quite realistic.

LDA seeks to find groups of related words. 
It is an iterative, generative algorithm with two main steps:

* During initialization, each word is assigned to a random topic.

* The algorithm goes through each word iteratively and reassigns the word to a topic with the following considerations:
    + the probability the word belongs to a topic;
    + the probability the document will be generated by a topic.

The concept behind the LDA topic model is that words belonging to a topic appear together in documents with high probability. 
It tries to model each document as a mixture of topics and each topic as a mixture of words.
This is sometimes referred to as a mixed-membership model. 

LDA attempts to find the mixture of words that is associated with each topic whilst at the same time determining the mixture of topics contained in a document. 
Then, the probability that a document belongs to a particular topic can be used to classify it accordingly. 

If the writer of the document from the original data is known, a recommendation of an artist/author based on similar topic structures can be made.

See <https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf> for more details on the LDA algorithm.

Alternatively, see <http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html#content> and references therein.


# An example of topic modelling with LDA

We will use a data set, `AssociatedPress`, which comes with the `topicmodels` package. This data set is a collection of 2,246 news articles published by AP mostly in 1988. In this data set, we will assume that each article is a document.

```{r}
data("AssociatedPress", package = "topicmodels")
AssociatedPress
```

As we can see the object AssociatedPress is a document-term matrix.

A document-term matrix (DTM) is an object in which **each document is a row**, and **each column is a term**. 
The value in row $i$ and column $j$ represents the frequency (number of times) with which term $j$ appears in document $i$. 
This format is required for the LDA algorithm.

The DTM is clearly not a tidy format and viceversa.

The `tidytext` package has two functions useful to convert from one format into the other:

* `tidy()` turns a DTM into a tidy data frame.

* `cast_dtm()` turns a tidy tibble (one token per row) into a DTM.

When a document-term pair doesn't occur then the value zero in entered. If there are many zeroes, we say the matrix has high sparsity. The `AssociatedPress` data set has 99% sparsity meaning that 99% of the matrix entries are zeroes.

The package `tm` has a function called `Terms()` which extracts the terms in the DTM to a vector

```{r}
AP_terms <- Terms(AssociatedPress)
glimpse(AP_terms)
```

To turn the DTM `AssociatedPress` into a tidy text object we use the function `tidy()`

```{r}
AP_tidy <- tidy(AssociatedPress)
AP_tidy
```

Note that terms with count zero are not included in the tidy version of the DTM.

Let us try and discover topics in this data set.

The function `LDA()` from the package `topicmodels` can be used to create a $k$ topic model. 
We will use $k = 2$ to generate a two-topic model for the AssociatedPress data.

This will take a bit long.

```{r cache=TRUE}
AP_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
```

```{r}
AP_lda
```

The object `AP_lda`contains all the details of the fitted model: how words are associated with topics and how topics are associated with documents. Let us now find that out.

## Words associated with topics

The model estimates per-topic-per-word probabilities, called $\beta$. We can obtain these $\beta$-values as follows

```{r}
AP_topics <- tidy(AP_lda, matrix = "beta")
AP_topics
```

Let us find out what the 20 most probable words for each topic are.

```{r}
AP_top20 <- AP_topics %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta))

AP_top20
```


Let us visualise the information

```{r}
AP_top20 %>% 
  ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~topic, scales = "free") +
  labs(x = "") +
  coord_flip()

```
top_terms %>%
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  coord_flip()
  

<br>

**EXERCISE.** Given the words most associated with each topic, how would you label the topics? Can you find any common words between the topics?

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

Topic 1 seems to be related to business and financial news whereas Topic 2 is related to political news.

"people" and "new" are some common words in the two topics.

<br>

In order to explore the words associated with the topics we could also look at the "greatest difference in $\beta$" between the topics as measured by the log of the ratio of a word's beta values:

$$LR = \log_2\left(\frac{\beta_2}{\beta_1}\right),$$
where the word has probability  $beta_1$ of being in topic 1 and probability $\beta_2$ of being in topic 2.

If $\beta_2 = 2\beta_1$, the word is twice as likely to be related to 2 than topic 1, then $LR = 1$, and if $\beta_1 = 2 \beta_2$, the word is twice as likely to be associated to topic 1 than topic 2, then 
$LR = -1$. 

Generally, if $\beta_2 > \beta_1$, then $LR > 0$ (association of word is more likely with topic 2 than topic 1) and if $\beta_2 < \beta_1$ then $LR < 0$ (association is more with topic 1 than topic 2). 

Let us explore words in `AssociatedPress` that have at least one of  $\beta_1$, $\beta_2$, greater than or equal to $1/1000$. 

```{r}
AP_topics
```


```{r}
AP_LR <- AP_topics %>%
  mutate(beta_value = paste0("beta", topic)) %>%
  select(-topic) %>%
  pivot_wider(names_from = beta_value, values_from = beta) %>%
  filter(beta1 > 0.001 | beta2 > 0.001) %>%
  mutate(LR = log2(beta2 / beta1))

AP_LR
```

**EXERCISE.** Create a bar plot of the 10 largest and 10 smallest LR. Interpret the graph

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

**Solution**

```{r}
aux1 <- AP_LR %>% 
  top_n(10, LR) %>% 
  mutate(dominant_topic = "political news") %>% 
  arrange(desc(LR))
aux1
```

```{r}
aux2 <- AP_LR %>%
  top_n(-10, LR) %>% 
  mutate(dominant_topic = "business and finance") %>% 
  arrange(desc(LR))
aux2
```

```{r}
bind_rows(aux1,aux2) %>% 
  ggplot(aes(x = reorder(term, LR), y = LR, fill = dominant_topic)) +
  geom_col() +
  labs(x = "Term") +
  coord_flip() 
```


## Topics associated with documents

One of the LDA model assumptions is that each document is a combination of topics. The output of the `LDA()` function contains information about the "per-document-per-topic" probabilities, a parameter called $\gamma$:

```{r}
AP_documents <- tidy(AP_lda, matrix = "gamma")
AP_documents
```

$ The parameter $\gamma$ represents an estimated proportion of words from the document that are generated from a topic. 
For example, about 24.8% of words in document 1 are generated from topic 1. And for document 6, it is mostly associated with topic 2.

```{r}
filter(AP_documents, document == 6)
```

What are the most common words in document 6? 

```{r}
AP_tidy %>%
  filter(document == 6) %>%
  arrange(desc(count))
```

Document 6 seems to be a news article that deals with America - Panama relations (Noriega was a dictator of Panama overthrown by a US invation to Panama). So the document is well classified into topic 2 mostly.

## Model selection

In order to fit a LDA model, we must provide the number of topics in the corpus, $k$. There are situations where we have a more or less clear idea about the number and even name of the different topics in a set of documents. However, this is not the most common situation in topic modelling. In this section we explore methods that can help us determine the $k$ parameter.

<br>

In order to evaluate and compare different probabilistic models, the **perplexity score** is commonly used, [D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of Machine Learning Research, vol. 3, no. 3, pp. 993–1022, 2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). 
This is usually done by splitting the dataset into two parts: one for training, the other for testing. So, we have the training corpus, $D_{train}$ and the test corpus, $D_{test}$.

The LDA model is fitted using the training data. This allows the computation of the (log-)likelihood, based on $D_{train}$, for the new set of documents $D_{test}$.

The formula to compute the perplexity score of a model is below. $p(w_d)$ is the likelihood
the model assigned to finding the words in document $d$ and $\sum_{d=1}^M  N_d$ is the total number of tokens or (non-unique) words in the test corpus $D_{test}$.

$$
perplexity(D_{test}) = \exp\left( - \frac{\sum_{d=1}^M \log p(w_d)}{\sum_{d=1}^M N_d} \right)
$$

Clearly, lower values of perplexity are preferred. A low log-likelihood would give high values of perplexity.

One could, of course, make the train and test set the same, in which case the perplexity is just equivalent to using the log-likelihhod as a criterion to choose between models.
Let us see how model selection in this situation would work.

The package `topicmodels` has a function to compute the perplexity, called `perplexity()`. Let us compute the perplexity for all the data. Recall `AP_lda` is the "TopicModel" object (`LDA()` function output).

```{r}
perplexity_all_k2 <- perplexity(AP_lda)
perplexity_all_k2
```

This number on its own doesn't say much.
Let us compute now the perplexity of the LDA model with 3 topics

```{r cache=T}
AP_lda_k3 <- LDA(AssociatedPress, k = 3, control = list(seed = 1234))
perplexity_all_k3 <- perplexity(AP_lda_k3)
perplexity_all_k3
```

```{r cache=T}
date()
AP_lda_k10 <- LDA(AssociatedPress, k = 10, control = list(seed = 1234))
perplexity_all_k10 <- perplexity(AP_lda_k10)
perplexity_all_k10
date()
```

```{r cache=T}
date()
AP_lda_k20 <- LDA(AssociatedPress, k = 20, control = list(seed = 1234))
perplexity_all_k20 <- perplexity(AP_lda_k20)
perplexity_all_k20
date()
```

```{r cache=T}
date()
AP_lda_k30 <- LDA(AssociatedPress, k = 30, control = list(seed = 1234))
perplexity_all_k30 <- perplexity(AP_lda_k30)
perplexity_all_k30
date()
```



We see that the perplexity is lower, therefore it seems that the LDA model with 3 topics makes the data more likely to have been observed.

Let us compute the perplexity for several values of $k$ and draw a curve perplexity vs. $k$. The following calculation will take a while

```{r cache=TRUE}
k <- c(2, 10, 20, 30, 40, 50, 60)
#
perplex_comp <- function(k){
  lda_fit <- LDA(AssociatedPress, k, control = list(seed = 1234))
  perplex <- perplexity(lda_fit)
perplex
}
#  
perplex_score <- purrr::map_dbl(k, perplex_comp)
#
perplex_score
```

```{r}
tibble(k, perplex_score) %>%
  ggplot(aes(x = k, y = perplex_score)) +
  geom_point() +
  geom_line()
```

```{r cache=T}
date()
AP_lda_k70 <- LDA(AssociatedPress, k = 70, control = list(seed = 1234))
perplexity_all_k70 <- perplexity(AP_lda_k70)
perplexity_all_k70
date()
```

```{r}
perplex_score
```

It seems that the perplexity evaluated on the entire data set is even lower at $k=70$.

<br>

The best use of the perplexity score is using cross-validation.

For example, we could separate the total number of available documents into two groups:

* A random selection of 80% of documents will be the training data, and the rest will be the test data.

```{r}
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
nr_docs
#
train_docs <- sample(1:nr_docs, 0.8*nr_docs)
test_docs <- setdiff(1:nr_docs, train_docs)

#
AP_tidy_train <- AP_tidy %>% filter(document %in% train_docs)
AP_tidy_test <- AP_tidy %>% filter(document %in% test_docs)

AP_tidy_train
```

Now we fit the model to the train set and compute the perplexity

```{r cache=T}
date()
AssociatedPress_train <- cast_dtm(AP_tidy_train, document, term, count)
AssociatedPress_test <- cast_dtm(AP_tidy_test, document, term, count)
AP_lda_k2 <- LDA(AssociatedPress_train, k = 2, control = list(seed = 1234))
perplexity_test_k2 <- perplexity(AP_lda_k2, newdata = AssociatedPress_test)
perplexity_test_k2
date()
```

which is larger than the perplexity when the train set is the test set as well (verify this).

Let us repeat the exercise

```{r}
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
#
train_docs <- sample(1:nr_docs, 0.8*nr_docs)
test_docs <- setdiff(1:nr_docs, train_docs)

#
AP_tidy_train <- AP_tidy %>% filter(document %in% train_docs)
AP_tidy_test <- AP_tidy %>% filter(document %in% test_docs)
```

```{r cache=T}
date()
AssociatedPress_train <- cast_dtm(AP_tidy_train, document, term, count)
AssociatedPress_test <- cast_dtm(AP_tidy_test, document, term, count)
AP_lda_k2 <- LDA(AssociatedPress_train, k = 2, control = list(seed = 1234))
perplexity_test_k2 <- perplexity(AP_lda_k2, newdata = AssociatedPress_test)
perplexity_test_k2
date()
```

When a different subset of documents is chosen as the train set, then the perplexity on the test set is different.

In order to avoid this problem of a sample-dependent perplexity score, we will carry out a $x$-fold cross validation exercise:

* Randomly split the data set into $x$ groups or folds.

* Choose one group as a test set and the remaining $x-1$ as a training set.

* Compute the perplexity of the test set.

* Repeat $x$ times, each time choosing a different test set.

* Average out the perplexities obtained in each the $x$ steps above.

<br>

Let us carry out a 4-fold cross-validation to get the perplexity value for the model with 2 topics.

```{r}
#create a vector with the unique document numbers and randomly shuffle the numbers
# we will set a seed for reproducibility
set.seed(123)
docs <- AP_tidy %>% select(document) %>% unique() %>% pull() %>% sample()
docs[1:10]
```

```{r}
#folds is a vector with entries that indicate a group
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
folds <- cut(1:nr_docs, breaks = 10, labels = FALSE)

#Perform 4-fold cross validation
perplexity_k2_1 <- 0
date()
for(i in 1:10){
    #Segment your data by fold using the which() function 
    doc_nr_test <- docs[which(folds == i, arr.ind = TRUE)]
    testData <- AP_tidy %>% filter(document %in% doc_nr_test)
    trainData <- AP_tidy %>% filter(!document %in% doc_nr_test)
    testData_dtm <- cast_dtm(testData, document, term, count)
    trainData_dtm <- cast_dtm(trainData, document, term, count)
    #
    lda_fit <- LDA(trainData_dtm, k = 2, control = list(seed = 1234))
    perplexity_k2_1[i] <- perplexity(lda_fit, newdata = testData_dtm)
}
date()
```

```{r}
mean(perplexity_k2_1)
```

Let us repeat

```{r}
#create a vector with the unique document numbers and randomly shuffle the numbers
# set a different seed
set.seed(456)
docs <- AP_tidy %>% select(document) %>% unique() %>% pull() %>% sample()
docs[1:10]
```

```{r}
#folds is a vector with entries that indicate a group
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
folds <- cut(1:nr_docs, breaks = 10, labels = FALSE)

#Perform 4-fold cross validation
perplexity_k2_2 <- 0
for(i in 1:10){
    doc_nr_test <- docs[which(folds == i, arr.ind = TRUE)]
    testData <- AP_tidy %>% filter(document %in% doc_nr_test)
    trainData <- AP_tidy %>% filter(!document %in% doc_nr_test)
    testData_dtm <- cast_dtm(testData, document, term, count)
    trainData_dtm <- cast_dtm(trainData, document, term, count)
    #
    lda_fit <- LDA(trainData_dtm, k = 2, control = list(seed = 1234))
    perplexity_k2_2[i] <- perplexity(lda_fit, newdata = testData_dtm)
}
```

```{r}
mean(perplexity_k2_2)
```

Again, we see that there is variation due to the particular initial partition of the set of documents, but the variation should be less.

We should repeat this process a few times to diffuse the effect of random partitions.

For example we could do a 10-times 10-fold cross validation:

```{r cache=TRUE}
#the following function returns the average perplexity score for a 10-fold cv
ten_fold_cv <- function(docs, folds, kk){
perplexity_score<- 0
for(i in 1:10){
    doc_nr_test <- docs[which(folds == i, arr.ind = TRUE)]
    testData <- AP_tidy %>% filter(document %in% doc_nr_test)
    trainData <- AP_tidy %>% filter(!document %in% doc_nr_test)
    testData_dtm <- cast_dtm(testData, document, term, count)
    trainData_dtm <- cast_dtm(trainData, document, term, count)
    #
    lda_fit <- LDA(trainData_dtm, k = kk, control = list(seed = 1234))
    perplexity_score[i] <- perplexity(lda_fit, newdata = testData_dtm)
} 
mean(perplexity_score)
}
```

```{r cache=TRUE}
date()
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
folds <- cut(1:nr_docs, breaks = 10, labels = FALSE)
perplexity_ave <- 0
#
for (j in 1:10){
set.seed(j)
docs <- AP_tidy %>% select(document) %>% unique() %>% pull() %>% sample()
perplexity_ave[j] <- ten_fold_cv(docs, folds, 2)
}
mean(perplexity_ave)
date()
```

Let us do this for $k=3$.

```{r cache=TRUE}
date()
nr_docs <- AP_tidy %>% select(document) %>% n_distinct()
folds <- cut(1:nr_docs, breaks = 10, labels = FALSE)
perplexity_ave <- 0
#
for (j in 1:10){
set.seed(j)
docs <- AP_tidy %>% select(document) %>% unique() %>% pull() %>% sample()
perplexity_ave[j] <- ten_fold_cv(docs, folds, 3)
}
mean(perplexity_ave)
date()
```

We see that the perplexity has decreased using $k=3$ (3 topics).

Ideally, we should do the above for several values of $k$ and find, if possible the smallest value of $k$ such that the perplexity is minimum.

We will not do this here as with higher values of $k$ the amount of time it takes for the LDA model to run is quite high.

